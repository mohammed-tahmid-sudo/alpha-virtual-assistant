{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = json.load(open(\"dataset_600.json\", 'r'))\n",
        "print(data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJAJF1bpitQ8",
        "outputId": "35323e38-f9af-47c1-a441-977419a79d87"
      },
      "id": "yJAJF1bpitQ8",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': \"What's the current weather in Dhaka?\", 'output': 'TRUE: Current weather in Dhaka'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJe61q-oi5iD",
        "outputId": "4c265837-7ba1-4654-af2a-b8b994cc5b72"
      },
      "id": "RJe61q-oi5iD",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.8.10-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/52.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.22.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Collecting unsloth_zoo>=2025.8.9 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.8.9-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.31-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.55.4)\n",
            "Collecting datasets<4.0.0,>=3.4.1 (from unsloth)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.34.4)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (0.21.4)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.8.9->unsloth) (0.10.0)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.8.9->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.8.9->unsloth) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.8.9->unsloth)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.8.10-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.22.1-py3-none-any.whl (544 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.8.9-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m196.0/196.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.31-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shtab, msgspec, tyro, xformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed bitsandbytes-0.47.0 cut_cross_entropy-25.1.1 datasets-3.6.0 msgspec-0.19.0 shtab-1.7.2 trl-0.22.1 tyro-0.9.31 unsloth-2025.8.10 unsloth_zoo-2025.8.9 xformers-0.0.32.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n",
        "print(f\"USING DEVICE {device}\")\n",
        "\n",
        "if device == \"CUDA\":\n",
        "  print(\"USING\", torch.cuda.get_device_name() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agnLgtxFjQGH",
        "outputId": "0cf195d3-ad07-4431-e075-fc0b67262954"
      },
      "id": "agnLgtxFjQGH",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING DEVICE CUDA\n",
            "USING Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "unsloth.WANDB_DISABLED = True\n",
        "model_name = \"unsloth/SmolLM-135M\"\n",
        "max_seq_length = 512\n",
        "dtype = None\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380,
          "referenced_widgets": [
            "c917e25da2c54eaca8d06e6d4bed800a",
            "c66ab1c6bf0343bba0d5267ecd5f5c22",
            "d7827e080c18407cb30b86896ffa8823",
            "8eecc934154f4ff3bf3e18808c02b98c",
            "30b483e9089d4fec977ba26c8531247f",
            "7298c7cb9c3e47fc97e5eb86b0af8e7e",
            "e3a1da1f8cfa4e4a8cc4b13abc3a4d22",
            "c5b3a34023bf4ed993d2367289621c69",
            "ebbedbbee7bc4752ada0d7804bd1f1a6",
            "7b98740735b54679bd3c39da95debe38",
            "5177d7c5e92941c0a03860114a7867cc",
            "5d813c4322ac4bfa9846a9b625b04633",
            "1e12c7fb63ac4731a2291d5e3d82743f",
            "e33e4f1901944791b8b11b5ed8cc2ffd",
            "019cf8d3dcf0405a91c2321a568fbf1f",
            "fb39b5a69928466787d7ae30b32d9789",
            "eaf06b5179f54d26bfb0d4265dcd6876",
            "e8a2b4566e984126bc094af5d0a6bd82",
            "098e6f3a38d643f7a2401f81622ae430",
            "8379f4fba4954c6eab458759ba7aad7e",
            "eb689eeace5241ada86c8a878bb15b81",
            "4a25bf46f0604ecc845af892a758f1e5",
            "359ed3939e3347a3b9a64cc9e5888ffd",
            "439bf074aff24429bf635d28a0cd48b8",
            "78b85db43347425786dd7af06a289657",
            "4b0b90d9dbac40d89b5e26c9b23dc6ac",
            "5cbfed3f39764bea91e72ba2213ef492",
            "2ad87842d2ca40a192ba6c408dea1092",
            "70cdbee1584941be8932b8e4bff06f6f",
            "84d056f53fb142c887863747f93dd3ff",
            "6b060bcbe43e421d8772839c11091fa8",
            "c369ce8753044ea28fc01570788ae870",
            "4cfc933897564e0ca6131c8ea30fee09",
            "f5284d85835f40d2a29b4cbc72b53c6c",
            "7aa975ad084f4a87b06f3ed0fd5ac364",
            "8c7ae4ad71354843beed38c0bb64cf5c",
            "1bac5d8ccab84c2393a024caa90b39a8",
            "27fa008fdf9a414fb234a2bae604cee9",
            "8e9aaedc2ed846be85893073fe724760",
            "e73815e4a0c14b48bf6e849b3ed50c53",
            "99478d48ac294330ac2d0c2e655548c5",
            "4bd14adf11f1458b8260a04f12b6da36",
            "7388fc6bc48947339f792e588ae7f6f1",
            "349e71b5d16e4821b4bffc63e2755bc2",
            "2054cc1347fe4cbaa420285033188ed2",
            "0e2846a3de5a4410bef57b823a1008b3",
            "00dfc6dd9bfa45a28ff7aa1a283ada0b",
            "025260cd0d904ebda19df619ec1da5bf",
            "87f7784af4074af2a51a60f70c45fde2",
            "880653a76363405da8a7f0d28c06cdb3",
            "a391ce5aae0749838a9337c3ea4fc1b6",
            "be6936ba027e41ab9354379b256c82ba",
            "b583ef8ed5ba49fa96f538fea4000061",
            "7d96967da18441f39b1ca8e327eb51a8",
            "848e55f00d4d405eb3a859de4ff001ee",
            "d923f59e3fc9446583adf4c884796017",
            "3af44a57f6fd4892935bcc076485b52a",
            "ba08fb9980cc4005a4694935d78500b2",
            "8537b92468524e4cb9f2e9359431b0d2",
            "65e6bb001c714b459e98f27df74343a5",
            "6e227659d780402db14dca67f1c6895a",
            "f7cc9c9fbe5c4c649dcc6081ec62d48d",
            "c8bb52bda28842fba2e986a70f3ebc02",
            "aa7d4bfd6ef44083ac6fa17ba73ae77b",
            "8db1c5d30f8f46d3959fe45526205da4",
            "9524fe27c45b4a5d8f2eeb169ddfa7fc",
            "73a419ddafc44b16a49c4b3724f308e9",
            "1cb54dd2e3e9427d8dfbbe47de18683a",
            "20bfb01bcdb34885b2d1b45e62569a73",
            "1f716f96d3344ed68ce2fd4c9124ea5d",
            "9a6e4e82e2ff43ab929472454c756588",
            "bce2d7fcbad2410abdaffb059083f917",
            "f4bbffa9dd2243f5a187b7b67370520b",
            "898bcd8e609f4e9bb712d44293154b4e",
            "417c1ecd2b7745c287c15be9da7f5238",
            "d26abe36907645c3b2ecc4905f6e39d8",
            "adbb05ce8c4d4dd8b356ced8ddb1dd7e"
          ]
        },
        "id": "VBL1USjAjWvi",
        "outputId": "d650c3dd-64fb-4343-95ec-3ce970471227"
      },
      "id": "VBL1USjAjWvi",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ε Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Ε Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.8.10: Fast Llama patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/112M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c917e25da2c54eaca8d06e6d4bed800a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d813c4322ac4bfa9846a9b625b04633"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "359ed3939e3347a3b9a64cc9e5888ffd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5284d85835f40d2a29b4cbc72b53c6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2054cc1347fe4cbaa420285033188ed2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/978 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d923f59e3fc9446583adf4c884796017"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73a419ddafc44b16a49c4b3724f308e9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def format_prompt(example):\n",
        "    return f\"### Input: {example['input']}\\n### Output: {json.dumps(example['output'])}<|endoftext|>\"\n",
        "\n",
        "formatted_data = [format_prompt(item) for item in data]\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})"
      ],
      "metadata": {
        "id": "gD3Vvgmvlip5"
      },
      "id": "gD3Vvgmvlip5",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=128,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn5137Ynn0DK",
        "outputId": "a56108e3-31b2-46f3-bd0d-c9db54ff7524"
      },
      "id": "Mn5137Ynn0DK",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.8.10 patched 30 layers with 30 QKV layers, 30 O layers and 30 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=25,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        dataloader_pin_memory=False,\n",
        "    ),\n",
        ")\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754,
          "referenced_widgets": [
            "da6e378d5fe64ec3bb0758f6595c6ba3",
            "1ad03fdc3cec471e8fb581435cefab82",
            "5d8285a7a5b943d8900aaca4233d1b3c",
            "e2276b1005164826ad5a65b5ed7eefaf",
            "cd3769f470da42898b8ab168ab56e60c",
            "34970ac525d0406fb18ee808aabbf873",
            "d087f1ad10ff48c4b7219fa53d7dd4e0",
            "7a8f72f37d324efab1b8476b43eaefae",
            "93d4bf8a68504367bc5b2aff18b747e2",
            "d4079f8a64ab408eaaa5d0b4b71bd59c",
            "e9addf9c2bbd43ba8f9968e403505143"
          ]
        },
        "id": "fraSOeZqoQEZ",
        "outputId": "5e05a70a-55b3-43f0-f7c2-a432b548a266"
      },
      "id": "fraSOeZqoQEZ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da6e378d5fe64ec3bb0758f6595c6ba3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 600 | Num Epochs = 3 | Total steps = 225\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 19,537,920 of 154,052,928 (12.68% trained)\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammedtahmid-bd\u001b[0m (\u001b[33mmohammedtahmid-bd-wertg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250901_163519-36j8ad64</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohammedtahmid-bd-wertg/huggingface/runs/36j8ad64' target=\"_blank\">iconic-dew-4</a></strong> to <a href='https://wandb.ai/mohammedtahmid-bd-wertg/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mohammedtahmid-bd-wertg/huggingface' target=\"_blank\">https://wandb.ai/mohammedtahmid-bd-wertg/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mohammedtahmid-bd-wertg/huggingface/runs/36j8ad64' target=\"_blank\">https://wandb.ai/mohammedtahmid-bd-wertg/huggingface/runs/36j8ad64</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [225/225 05:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.878200</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.254300</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.182800</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.162800</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.169700</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.159200</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.155300</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXNDEwJUpDhZ",
        "outputId": "d4dee4c0-7d47-4a3c-92b1-1acdbb9779f2"
      },
      "id": "eXNDEwJUpDhZ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(49152, 576, padding_idx=16)\n",
              "        (layers): ModuleList(\n",
              "          (0-29): 30 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=576, out_features=576, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=576, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=576, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=576, out_features=192, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=576, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=576, out_features=192, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=576, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=576, out_features=576, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=576, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=576, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=576, out_features=1536, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=576, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=576, out_features=1536, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=576, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=1536, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=1536, out_features=576, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=1536, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=576, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-using the format_prompt function defined earlier\n",
        "def format_prompt(example):\n",
        "    return f\"### Input: {example['input']}\\n### Output:\"\n",
        "\n",
        "messages = [\n",
        "    {\"input\": \"open up firefox\"},\n",
        "]\n",
        "# Apply the custom formatting to the input\n",
        "formatted_input = format_prompt(messages[0])\n",
        "\n",
        "inputs = tokenizer(\n",
        "    formatted_input,\n",
        "    tokenize=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0BWvqvRMrnTT",
        "outputId": "369af633-6a8d-486e-9188-dbb73d9a731e"
      },
      "id": "0BWvqvRMrnTT",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'tokenize'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1160451513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mformatted_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m inputs = tokenizer(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mformatted_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2908\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2910\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2911\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3018\u001b[0m             )\n\u001b[1;32m   3019\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3020\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   3021\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3093\u001b[0m         )\n\u001b[1;32m   3094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3095\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   3096\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3097\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     ) -> BatchEncoding:\n\u001b[1;32m    626\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'tokenize'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-bMUNxVr3Bb"
      },
      "id": "O-bMUNxVr3Bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b079fc21"
      },
      "source": [
        "You can save the model and tokenizer locally:"
      ],
      "id": "b079fc21"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ee4600c"
      },
      "source": [
        "model.save_pretrained(\"finetuned_smollm\")\n",
        "tokenizer.save_pretrained(\"finetuned_smollm\")"
      ],
      "id": "4ee4600c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20be6708"
      },
      "source": [
        "Or push them to the Hugging Face Hub (replace \"your_username\" with your actual username):"
      ],
      "id": "20be6708"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43135555"
      },
      "source": [
        "# from huggingface_hub import login\n",
        "# login() # You'll be prompted to enter your Hugging Face token\n",
        "\n",
        "# model.push_to_hub(\"your_username/finetuned_smollm\")\n",
        "# tokenizer.push_to_hub(\"your_username/finetuned_smollm\")"
      ],
      "id": "43135555",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80e0ab8c"
      },
      "source": [
        "To load the saved model and tokenizer for inference:"
      ],
      "id": "80e0ab8c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd4a85c3"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the model and tokenizer from the saved directory\n",
        "model_path = \"finetuned_smollm\" # Replace with the path where you saved your model\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path, # Load from the local path\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# You can also load the model for inference directly\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "id": "dd4a85c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e51fc37"
      },
      "source": [
        "Now you can use the loaded model and tokenizer for inference, just like before."
      ],
      "id": "1e51fc37"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92bdad2a"
      },
      "source": [
        "# Re-using the format_prompt function defined earlier\n",
        "def format_prompt(example):\n",
        "    return f\"### Input: {example['input']}\\n### Output:\"\n",
        "\n",
        "messages = [\n",
        "    {\"input\": \"if 2x = 6+9 then solve for x\"},\n",
        "]\n",
        "# Apply the custom formatting to the input\n",
        "formatted_input = format_prompt(messages[0])\n",
        "\n",
        "inputs = tokenizer(\n",
        "    formatted_input,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True, # Add padding\n",
        "    truncation=True, # Add truncation\n",
        "    max_length=max_seq_length, # Specify max_length\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"], # Explicitly pass input_ids\n",
        "    attention_mask=inputs[\"attention_mask\"], # Explicitly pass attention_mask\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)"
      ],
      "id": "92bdad2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQ9fNwAxscqO"
      },
      "id": "OQ9fNwAxscqO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d691e150"
      },
      "source": [
        "# Task\n",
        "Rewrite the provided Python code for finetuning a language model using PEFT and SFTTrainer, making it cleaner and more organized. The rewritten code should include sections for setup and data loading, model loading, data preparation, model training, inference, and saving/loading the finetuned model."
      ],
      "id": "d691e150"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690bf1e9"
      },
      "source": [
        "## Setup and data loading\n",
        "\n",
        "### Subtask:\n",
        "Combine necessary imports and data loading into a single section.\n"
      ],
      "id": "690bf1e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0337b0e"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine all the necessary imports and data loading steps into a single code block as per the instructions.\n",
        "\n"
      ],
      "id": "e0337b0e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "befc4143"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "data = json.load(open(\"dataset_600.json\", 'r'))\n",
        "print(data[0])\n",
        "\n",
        "device = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n",
        "print(f\"USING DEVICE {device}\")\n",
        "\n",
        "if device == \"CUDA\":\n",
        "  print(\"USING\", torch.cuda.get_device_name() )"
      ],
      "id": "befc4143",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa4433e9"
      },
      "source": [
        "## Model loading\n",
        "\n",
        "### Subtask:\n",
        "Load the pre-trained model and tokenizer.\n"
      ],
      "id": "aa4433e9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed12827c"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the pre-trained model and tokenizer using FastLanguageModel.from_pretrained with the specified parameters, including 4-bit quantization.\n",
        "\n"
      ],
      "id": "ed12827c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "783388d6",
        "outputId": "b817e8dc-d665-4ca8-b5e5-499c5aa78a86"
      },
      "source": [
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "unsloth.WANDB_DISABLED = True\n",
        "model_name = \"unsloth/SmolLM-135M\"\n",
        "max_seq_length = 512\n",
        "dtype = None\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True)"
      ],
      "id": "783388d6",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.10: Fast Llama patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0e8bfa"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Format the data for training and create a Hugging Face Dataset.\n"
      ],
      "id": "2d0e8bfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60826843"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires formatting the data and creating a Hugging Face Dataset. This involves defining a formatting function, applying it to the data, and then creating the dataset.\n",
        "\n"
      ],
      "id": "60826843"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "573fb433",
        "outputId": "13bc630d-92de-4ff1-cac4-cd2c6f5ef7db"
      },
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "def format_prompt(example):\n",
        "    return f\"### Input: {example['input']}\\n### Output: {json.dumps(example['output'])}<|endoftext|>\"\n",
        "\n",
        "formatted_data = [format_prompt(item) for item in data]\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "print(dataset)"
      ],
      "id": "573fb433",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 600\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bcc5a99"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Define the PEFT model and set up the SFTTrainer for finetuning.\n"
      ],
      "id": "2bcc5a99"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89c116fb"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the PEFT model and set up the SFTTrainer for finetuning.\n",
        "\n"
      ],
      "id": "89c116fb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476,
          "referenced_widgets": [
            "0122288a05f2498eb51772576989849f",
            "33cc249495bf4400870d6d04df73c80c",
            "10f226fa970a4bed93e04104481efa22",
            "a547b724e2f242839e217d34045d5777",
            "fe30f6a92a8545aebaeb2c7e5b52d133",
            "42fc9e010fa54100927d6b5078536d8f",
            "6b1ec1ebfc7e4988b57cf4c34b76f9b3",
            "5573f3b3df6d4b159ec883e9d5730220",
            "4e2c297d14b6461384191e470ad29e7b",
            "5995beb14e86492098084f63d7bb9689",
            "aa57fba2bbfa44d19fd578f49e6e7c15"
          ]
        },
        "id": "d22b1292",
        "outputId": "496306d5-0a18-4402-d073-0edc729c4100"
      },
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=128,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=25,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    dataloader_pin_memory=False,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "id": "d22b1292",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0122288a05f2498eb51772576989849f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 600 | Num Epochs = 3 | Total steps = 225\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 19,537,920 of 154,052,928 (12.68% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [225/225 05:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.878200</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.254300</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.182800</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.162800</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.169700</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.159200</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.155000</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.155300</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c6ca4ca"
      },
      "source": [
        "## Inference\n",
        "\n",
        "### Subtask:\n",
        "Prepare the model for inference and demonstrate how to generate responses.\n"
      ],
      "id": "1c6ca4ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4883022"
      },
      "source": [
        "**Reasoning**:\n",
        "Prepare the model for inference, define the prompt formatting function, create input messages, format the input, tokenize it, generate a response from the model, decode the response, and print it.\n",
        "\n"
      ],
      "id": "d4883022"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3fb02c4",
        "outputId": "84629090-afd2-412e-accd-eda85ba989ec"
      },
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def format_prompt(example):\n",
        "    return f\"### Input: {example['input']}\\n### Output:\"\n",
        "\n",
        "messages = [\n",
        "    {\"input\": \"if 2x = 6+9 then solve for x\"},\n",
        "    {\"input\": \"What's the current weather in London?\"}\n",
        "]\n",
        "\n",
        "for message in messages:\n",
        "    formatted_input = format_prompt(message)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        formatted_input,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=256,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "    print(response)"
      ],
      "id": "f3fb02c4",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Input: if 2x = 6+9 then solve for x\n",
            "### Output: \"TRUE: \"<|endoftext|>\n",
            "### Input: What's the current weather in London?\n",
            "### Output: \"TRUE: Current weather in London\"<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8d75a46"
      },
      "source": [
        "## Saving and loading\n",
        "\n",
        "### Subtask:\n",
        "Include steps for saving and loading the finetuned model and tokenizer.\n"
      ],
      "id": "e8d75a46"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdb7431b"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the finetuned model and tokenizer to a local directory.\n",
        "\n"
      ],
      "id": "bdb7431b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e510939",
        "outputId": "efbf8be2-5f84-4786-aab1-a10c9460efe2"
      },
      "source": [
        "model_save_path = \"finetuned_smollm\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model and tokenizer saved to {model_save_path}\")"
      ],
      "id": "5e510939",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to finetuned_smollm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68c56018"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the saved model and tokenizer and prepare the model for inference.\n",
        "\n"
      ],
      "id": "68c56018"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48cc50cd",
        "outputId": "a153c549-6874-44c6-ac51-40cb8163c761"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_path = \"finetuned_smollm\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(f\"Model and tokenizer loaded from {model_path} and prepared for inference.\")"
      ],
      "id": "48cc50cd",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.10: Fast Llama patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Model and tokenizer loaded from finetuned_smollm and prepared for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3660346a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The script successfully loaded data from a JSON file, identified the available device (CUDA or CPU), and loaded the pre-trained `unsloth/SmolLM-135M` model and tokenizer, utilizing 4-bit quantization.\n",
        "*   The data was correctly formatted into a prompt structure (`### Input: ...\\n### Output: ...<|endoftext|>`) and converted into a Hugging Face Dataset with a \"text\" column.\n",
        "*   The model was configured for PEFT training using LoRA with specified parameters ($\\text{r}=64$, $\\text{lora\\_alpha}=128$), and the SFTTrainer was successfully set up with defined training arguments (e.g., batch size 2, 3 epochs, learning rate $\\text{2e}^{-4}$).\n",
        "*   The training process was initiated and ran successfully.\n",
        "*   The finetuned model was prepared for inference, and the inference process was demonstrated by generating responses for example prompts.\n",
        "*   The finetuned model and tokenizer were successfully saved to a local directory and subsequently loaded back for inference.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The modular approach of separating the code into distinct sections (Setup/Data Loading, Model Loading, Data Preparation, Training, Inference, Saving/Loading) significantly improves code readability and organization, making it easier to understand and maintain the finetuning pipeline.\n",
        "*   The saved model and tokenizer can be easily deployed for inference or further fine-tuning, providing a clear pathway for utilizing the trained model in other applications.\n"
      ],
      "id": "3660346a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba4649ce"
      },
      "source": [
        "You can save the model and tokenizer locally:"
      ],
      "id": "ba4649ce"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b86cae9",
        "outputId": "e583ffe2-0061-4572-97df-02a5a44d8722"
      },
      "source": [
        "model.save_pretrained_gguf(\"pwd\", tokenizer, quantization_method=\"q4_k_m\")"
      ],
      "id": "5b86cae9",
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 111.9M\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 5.3 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 30/30 [00:00<00:00, 79.87it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving pwd/pytorch_model.bin...\n",
            "Done.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at pwd into f16 GGUF format.\n",
            "The output location will be /content/pwd/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: pwd\n",
            "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {576, 49152}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {576, 192}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {576, 576}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {576, 1536}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {1536, 576}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {576}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 2048\n",
            "INFO:hf-to-gguf:gguf: embedding length = 576\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 1536\n",
            "INFO:hf-to-gguf:gguf: head count = 9\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 3\n",
            "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 48900 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 0\n",
            "INFO:gguf.vocab:Setting special token type eos to 0\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 16\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/pwd/unsloth.F16.gguf: n_tensors = 272, total_size = 269.1M\n",
            "Writing: 100%|| 269M/269M [00:00<00:00, 659Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/pwd/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/pwd/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 6345 (4b20d8b7)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/pwd/unsloth.F16.gguf' to '/content/pwd/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 32 key-value pairs and 272 tensors from /content/pwd/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Pwd\n",
            "llama_model_loader: - kv   3:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 135M\n",
            "llama_model_loader: - kv   5:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv   6:                               general.tags arr[str,2]       = [\"unsloth\", \"llama.cpp\"]\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 30\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 576\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 1536\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 9\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 3\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = smollm\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,48900]   = [\" t\", \" a\", \"i n\", \"h e\", \" ...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 16\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:   61 tensors\n",
            "llama_model_loader: - type  f16:  211 tensors\n",
            "[   1/ 272]                   output_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[   2/ 272]                    token_embd.weight - [  576, 49152,     1,     1], type =    f16, converting to q8_0 .. size =    54.00 MiB ->    28.69 MiB\n",
            "[   3/ 272]                  blk.0.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[   4/ 272]               blk.0.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[   5/ 272]             blk.0.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[   6/ 272]                  blk.0.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[   7/ 272]                  blk.0.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[   8/ 272]                blk.0.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[   9/ 272]                blk.0.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  10/ 272]                blk.0.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  11/ 272]                  blk.0.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  12/ 272]                  blk.1.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  13/ 272]               blk.1.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  14/ 272]             blk.1.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  15/ 272]                  blk.1.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  16/ 272]                  blk.1.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[  17/ 272]                blk.1.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[  18/ 272]                blk.1.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  19/ 272]                blk.1.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  20/ 272]                  blk.1.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  21/ 272]                  blk.2.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  22/ 272]               blk.2.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  23/ 272]             blk.2.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  24/ 272]                  blk.2.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  25/ 272]                  blk.2.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[  26/ 272]                blk.2.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[  27/ 272]                blk.2.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  28/ 272]                blk.2.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  29/ 272]                  blk.2.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  30/ 272]                  blk.3.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  31/ 272]               blk.3.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  32/ 272]             blk.3.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  33/ 272]                  blk.3.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  34/ 272]                  blk.3.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  35/ 272]                blk.3.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[  36/ 272]                blk.3.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  37/ 272]                blk.3.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  38/ 272]                  blk.3.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  39/ 272]                  blk.4.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  40/ 272]               blk.4.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  41/ 272]             blk.4.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  42/ 272]                  blk.4.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  43/ 272]                  blk.4.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  44/ 272]                blk.4.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[  45/ 272]                blk.4.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  46/ 272]                blk.4.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  47/ 272]                  blk.4.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  48/ 272]                  blk.5.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  49/ 272]               blk.5.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  50/ 272]             blk.5.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  51/ 272]                  blk.5.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  52/ 272]                  blk.5.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[  53/ 272]                blk.5.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[  54/ 272]                blk.5.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  55/ 272]                blk.5.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  56/ 272]                  blk.5.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  57/ 272]                  blk.6.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  58/ 272]               blk.6.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  59/ 272]             blk.6.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  60/ 272]                  blk.6.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  61/ 272]                  blk.6.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  62/ 272]                blk.6.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[  63/ 272]                blk.6.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  64/ 272]                blk.6.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  65/ 272]                  blk.6.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  66/ 272]                  blk.7.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  67/ 272]               blk.7.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  68/ 272]             blk.7.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  69/ 272]                  blk.7.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  70/ 272]                  blk.7.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  71/ 272]                blk.7.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[  72/ 272]                blk.7.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  73/ 272]                blk.7.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  74/ 272]                  blk.7.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  75/ 272]                  blk.8.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  76/ 272]               blk.8.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  77/ 272]             blk.8.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  78/ 272]                  blk.8.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  79/ 272]                  blk.8.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[  80/ 272]                blk.8.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[  81/ 272]                blk.8.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  82/ 272]                blk.8.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  83/ 272]                  blk.8.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  84/ 272]                  blk.9.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  85/ 272]               blk.9.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  86/ 272]             blk.9.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  87/ 272]                  blk.9.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  88/ 272]                  blk.9.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  89/ 272]                blk.9.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[  90/ 272]                blk.9.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  91/ 272]                blk.9.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  92/ 272]                  blk.9.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[  93/ 272]                 blk.10.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  94/ 272]              blk.10.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  95/ 272]            blk.10.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  96/ 272]                 blk.10.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[  97/ 272]                 blk.10.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[  98/ 272]               blk.10.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[  99/ 272]               blk.10.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 100/ 272]               blk.10.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 101/ 272]                 blk.10.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 102/ 272]                 blk.11.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 103/ 272]              blk.11.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 104/ 272]            blk.11.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 105/ 272]                 blk.11.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 106/ 272]                 blk.11.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 107/ 272]               blk.11.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 108/ 272]               blk.11.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 109/ 272]               blk.11.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 110/ 272]                 blk.11.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 111/ 272]                 blk.12.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 112/ 272]              blk.12.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 113/ 272]            blk.12.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 114/ 272]                 blk.12.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 115/ 272]                 blk.12.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 116/ 272]               blk.12.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 117/ 272]               blk.12.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 118/ 272]               blk.12.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 119/ 272]                 blk.12.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 120/ 272]                 blk.13.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 121/ 272]              blk.13.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 122/ 272]            blk.13.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 123/ 272]                 blk.13.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 124/ 272]                 blk.13.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 125/ 272]               blk.13.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 126/ 272]               blk.13.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 127/ 272]               blk.13.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 128/ 272]                 blk.13.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 129/ 272]                 blk.14.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 130/ 272]              blk.14.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 131/ 272]            blk.14.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 132/ 272]                 blk.14.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 133/ 272]                 blk.14.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 134/ 272]               blk.14.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 135/ 272]               blk.14.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 136/ 272]               blk.14.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 137/ 272]                 blk.14.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 138/ 272]                 blk.15.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 139/ 272]              blk.15.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 140/ 272]            blk.15.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 141/ 272]                 blk.15.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 142/ 272]                 blk.15.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 143/ 272]               blk.15.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 144/ 272]               blk.15.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 145/ 272]               blk.15.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 146/ 272]                 blk.15.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 147/ 272]                 blk.16.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 148/ 272]              blk.16.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 149/ 272]            blk.16.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 150/ 272]                 blk.16.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 151/ 272]                 blk.16.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 152/ 272]               blk.16.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 153/ 272]               blk.16.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 154/ 272]               blk.16.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 155/ 272]                 blk.16.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 156/ 272]                 blk.17.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 157/ 272]              blk.17.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 158/ 272]            blk.17.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 159/ 272]                 blk.17.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 160/ 272]                 blk.17.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 161/ 272]               blk.17.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 162/ 272]               blk.17.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 163/ 272]               blk.17.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 164/ 272]                 blk.17.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 165/ 272]                 blk.18.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 166/ 272]              blk.18.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 167/ 272]            blk.18.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 168/ 272]                 blk.18.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 169/ 272]                 blk.18.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 170/ 272]               blk.18.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 171/ 272]               blk.18.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 172/ 272]               blk.18.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 173/ 272]                 blk.18.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 174/ 272]                 blk.19.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 175/ 272]              blk.19.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 176/ 272]            blk.19.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 177/ 272]                 blk.19.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 178/ 272]                 blk.19.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 179/ 272]               blk.19.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 180/ 272]               blk.19.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 181/ 272]               blk.19.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 182/ 272]                 blk.19.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 183/ 272]                 blk.20.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 184/ 272]              blk.20.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 185/ 272]            blk.20.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 186/ 272]                 blk.20.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 187/ 272]                 blk.20.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 188/ 272]               blk.20.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 189/ 272]               blk.20.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 190/ 272]               blk.20.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 191/ 272]                 blk.20.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 192/ 272]                 blk.21.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 193/ 272]              blk.21.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 194/ 272]            blk.21.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 195/ 272]                 blk.21.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 196/ 272]                 blk.21.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 197/ 272]               blk.21.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 198/ 272]               blk.21.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 199/ 272]               blk.21.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 200/ 272]                 blk.21.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 201/ 272]                 blk.22.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 202/ 272]              blk.22.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 203/ 272]            blk.22.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 204/ 272]                 blk.22.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 205/ 272]                 blk.22.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 206/ 272]               blk.22.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 207/ 272]               blk.22.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 208/ 272]               blk.22.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 209/ 272]                 blk.22.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 210/ 272]                 blk.23.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 211/ 272]              blk.23.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 212/ 272]            blk.23.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 213/ 272]                 blk.23.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 214/ 272]                 blk.23.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 215/ 272]               blk.23.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 216/ 272]               blk.23.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 217/ 272]               blk.23.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 218/ 272]                 blk.23.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 219/ 272]                 blk.24.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 220/ 272]              blk.24.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 221/ 272]            blk.24.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 222/ 272]                 blk.24.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 223/ 272]                 blk.24.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 224/ 272]               blk.24.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 225/ 272]               blk.24.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 226/ 272]               blk.24.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 227/ 272]                 blk.24.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 228/ 272]                 blk.25.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 229/ 272]              blk.25.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 230/ 272]            blk.25.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 231/ 272]                 blk.25.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 232/ 272]                 blk.25.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 233/ 272]               blk.25.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q4_K .. size =     1.69 MiB ->     0.47 MiB\n",
            "[ 234/ 272]               blk.25.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 235/ 272]               blk.25.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 236/ 272]                 blk.25.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 237/ 272]                 blk.26.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 238/ 272]              blk.26.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 239/ 272]            blk.26.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 240/ 272]                 blk.26.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 241/ 272]                 blk.26.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 242/ 272]               blk.26.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 243/ 272]               blk.26.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 244/ 272]               blk.26.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 245/ 272]                 blk.26.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 246/ 272]                 blk.27.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 247/ 272]              blk.27.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 248/ 272]            blk.27.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 249/ 272]                 blk.27.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 250/ 272]                 blk.27.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 251/ 272]               blk.27.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 252/ 272]               blk.27.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 253/ 272]               blk.27.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 254/ 272]                 blk.27.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 255/ 272]                 blk.28.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 256/ 272]              blk.28.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 257/ 272]            blk.28.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 258/ 272]                 blk.28.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 259/ 272]                 blk.28.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 260/ 272]               blk.28.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 261/ 272]               blk.28.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 262/ 272]               blk.28.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 263/ 272]                 blk.28.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 264/ 272]                 blk.29.attn_k.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.21 MiB ->     0.07 MiB\n",
            "[ 265/ 272]              blk.29.attn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 266/ 272]            blk.29.attn_output.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 267/ 272]                 blk.29.attn_q.weight - [  576,   576,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 576 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     0.63 MiB ->     0.22 MiB\n",
            "[ 268/ 272]                 blk.29.attn_v.weight - [  576,   192,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 192 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
            "converting to q8_0 .. size =     0.21 MiB ->     0.11 MiB\n",
            "[ 269/ 272]               blk.29.ffn_down.weight - [ 1536,   576,     1,     1], type =    f16, converting to q6_K .. size =     1.69 MiB ->     0.69 MiB\n",
            "[ 270/ 272]               blk.29.ffn_gate.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "[ 271/ 272]               blk.29.ffn_norm.weight - [  576,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 272/ 272]                 blk.29.ffn_up.weight - [  576,  1536,     1,     1], type =    f16, \n",
            "\n",
            "llama_tensor_get_type : tensor cols 576 x 1536 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
            "converting to q5_0 .. size =     1.69 MiB ->     0.58 MiB\n",
            "llama_model_quantize_impl: model size  =   256.63 MB\n",
            "llama_model_quantize_impl: quant size  =    98.87 MB\n",
            "llama_model_quantize_impl: WARNING: 180 of 211 tensor(s) required fallback quantization\n",
            "\n",
            "main: quantize time =  5510.24 ms\n",
            "main:    total time =  5510.24 ms\n",
            "Unsloth: Conversion completed! Output location: /content/pwd/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "821ebe6b"
      },
      "source": [
        "Or push them to the Hugging Face Hub (replace \"your_username\" with your actual username):"
      ],
      "id": "821ebe6b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "233a3e42"
      },
      "source": [
        "# from huggingface_hub import login\n",
        "# login() # You'll be prompted to enter your Hugging Face token\n",
        "print(\"hello world\")"
      ],
      "id": "233a3e42",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7KJv1KfTCJO"
      },
      "id": "P7KJv1KfTCJO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c917e25da2c54eaca8d06e6d4bed800a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c66ab1c6bf0343bba0d5267ecd5f5c22",
              "IPY_MODEL_d7827e080c18407cb30b86896ffa8823",
              "IPY_MODEL_8eecc934154f4ff3bf3e18808c02b98c"
            ],
            "layout": "IPY_MODEL_30b483e9089d4fec977ba26c8531247f"
          }
        },
        "c66ab1c6bf0343bba0d5267ecd5f5c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7298c7cb9c3e47fc97e5eb86b0af8e7e",
            "placeholder": "",
            "style": "IPY_MODEL_e3a1da1f8cfa4e4a8cc4b13abc3a4d22",
            "value": "model.safetensors:100%"
          }
        },
        "d7827e080c18407cb30b86896ffa8823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5b3a34023bf4ed993d2367289621c69",
            "max": 111879715,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebbedbbee7bc4752ada0d7804bd1f1a6",
            "value": 111879715
          }
        },
        "8eecc934154f4ff3bf3e18808c02b98c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b98740735b54679bd3c39da95debe38",
            "placeholder": "",
            "style": "IPY_MODEL_5177d7c5e92941c0a03860114a7867cc",
            "value": "112M/112M[00:02&lt;00:00,45.4MB/s]"
          }
        },
        "30b483e9089d4fec977ba26c8531247f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7298c7cb9c3e47fc97e5eb86b0af8e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3a1da1f8cfa4e4a8cc4b13abc3a4d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5b3a34023bf4ed993d2367289621c69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebbedbbee7bc4752ada0d7804bd1f1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b98740735b54679bd3c39da95debe38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5177d7c5e92941c0a03860114a7867cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d813c4322ac4bfa9846a9b625b04633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e12c7fb63ac4731a2291d5e3d82743f",
              "IPY_MODEL_e33e4f1901944791b8b11b5ed8cc2ffd",
              "IPY_MODEL_019cf8d3dcf0405a91c2321a568fbf1f"
            ],
            "layout": "IPY_MODEL_fb39b5a69928466787d7ae30b32d9789"
          }
        },
        "1e12c7fb63ac4731a2291d5e3d82743f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaf06b5179f54d26bfb0d4265dcd6876",
            "placeholder": "",
            "style": "IPY_MODEL_e8a2b4566e984126bc094af5d0a6bd82",
            "value": "generation_config.json:100%"
          }
        },
        "e33e4f1901944791b8b11b5ed8cc2ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098e6f3a38d643f7a2401f81622ae430",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8379f4fba4954c6eab458759ba7aad7e",
            "value": 111
          }
        },
        "019cf8d3dcf0405a91c2321a568fbf1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb689eeace5241ada86c8a878bb15b81",
            "placeholder": "",
            "style": "IPY_MODEL_4a25bf46f0604ecc845af892a758f1e5",
            "value": "111/111[00:00&lt;00:00,6.80kB/s]"
          }
        },
        "fb39b5a69928466787d7ae30b32d9789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaf06b5179f54d26bfb0d4265dcd6876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8a2b4566e984126bc094af5d0a6bd82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "098e6f3a38d643f7a2401f81622ae430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8379f4fba4954c6eab458759ba7aad7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb689eeace5241ada86c8a878bb15b81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a25bf46f0604ecc845af892a758f1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "359ed3939e3347a3b9a64cc9e5888ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_439bf074aff24429bf635d28a0cd48b8",
              "IPY_MODEL_78b85db43347425786dd7af06a289657",
              "IPY_MODEL_4b0b90d9dbac40d89b5e26c9b23dc6ac"
            ],
            "layout": "IPY_MODEL_5cbfed3f39764bea91e72ba2213ef492"
          }
        },
        "439bf074aff24429bf635d28a0cd48b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ad87842d2ca40a192ba6c408dea1092",
            "placeholder": "",
            "style": "IPY_MODEL_70cdbee1584941be8932b8e4bff06f6f",
            "value": "tokenizer_config.json:"
          }
        },
        "78b85db43347425786dd7af06a289657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d056f53fb142c887863747f93dd3ff",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b060bcbe43e421d8772839c11091fa8",
            "value": 1
          }
        },
        "4b0b90d9dbac40d89b5e26c9b23dc6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c369ce8753044ea28fc01570788ae870",
            "placeholder": "",
            "style": "IPY_MODEL_4cfc933897564e0ca6131c8ea30fee09",
            "value": "3.74k/?[00:00&lt;00:00,363kB/s]"
          }
        },
        "5cbfed3f39764bea91e72ba2213ef492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ad87842d2ca40a192ba6c408dea1092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70cdbee1584941be8932b8e4bff06f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84d056f53fb142c887863747f93dd3ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6b060bcbe43e421d8772839c11091fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c369ce8753044ea28fc01570788ae870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cfc933897564e0ca6131c8ea30fee09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5284d85835f40d2a29b4cbc72b53c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7aa975ad084f4a87b06f3ed0fd5ac364",
              "IPY_MODEL_8c7ae4ad71354843beed38c0bb64cf5c",
              "IPY_MODEL_1bac5d8ccab84c2393a024caa90b39a8"
            ],
            "layout": "IPY_MODEL_27fa008fdf9a414fb234a2bae604cee9"
          }
        },
        "7aa975ad084f4a87b06f3ed0fd5ac364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e9aaedc2ed846be85893073fe724760",
            "placeholder": "",
            "style": "IPY_MODEL_e73815e4a0c14b48bf6e849b3ed50c53",
            "value": "vocab.json:"
          }
        },
        "8c7ae4ad71354843beed38c0bb64cf5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99478d48ac294330ac2d0c2e655548c5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bd14adf11f1458b8260a04f12b6da36",
            "value": 1
          }
        },
        "1bac5d8ccab84c2393a024caa90b39a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7388fc6bc48947339f792e588ae7f6f1",
            "placeholder": "",
            "style": "IPY_MODEL_349e71b5d16e4821b4bffc63e2755bc2",
            "value": "801k/?[00:00&lt;00:00,298kB/s]"
          }
        },
        "27fa008fdf9a414fb234a2bae604cee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e9aaedc2ed846be85893073fe724760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e73815e4a0c14b48bf6e849b3ed50c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99478d48ac294330ac2d0c2e655548c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4bd14adf11f1458b8260a04f12b6da36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7388fc6bc48947339f792e588ae7f6f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349e71b5d16e4821b4bffc63e2755bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2054cc1347fe4cbaa420285033188ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e2846a3de5a4410bef57b823a1008b3",
              "IPY_MODEL_00dfc6dd9bfa45a28ff7aa1a283ada0b",
              "IPY_MODEL_025260cd0d904ebda19df619ec1da5bf"
            ],
            "layout": "IPY_MODEL_87f7784af4074af2a51a60f70c45fde2"
          }
        },
        "0e2846a3de5a4410bef57b823a1008b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_880653a76363405da8a7f0d28c06cdb3",
            "placeholder": "",
            "style": "IPY_MODEL_a391ce5aae0749838a9337c3ea4fc1b6",
            "value": "merges.txt:"
          }
        },
        "00dfc6dd9bfa45a28ff7aa1a283ada0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be6936ba027e41ab9354379b256c82ba",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b583ef8ed5ba49fa96f538fea4000061",
            "value": 1
          }
        },
        "025260cd0d904ebda19df619ec1da5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d96967da18441f39b1ca8e327eb51a8",
            "placeholder": "",
            "style": "IPY_MODEL_848e55f00d4d405eb3a859de4ff001ee",
            "value": "466k/?[00:00&lt;00:00,8.93MB/s]"
          }
        },
        "87f7784af4074af2a51a60f70c45fde2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880653a76363405da8a7f0d28c06cdb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a391ce5aae0749838a9337c3ea4fc1b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be6936ba027e41ab9354379b256c82ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b583ef8ed5ba49fa96f538fea4000061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d96967da18441f39b1ca8e327eb51a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848e55f00d4d405eb3a859de4ff001ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d923f59e3fc9446583adf4c884796017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3af44a57f6fd4892935bcc076485b52a",
              "IPY_MODEL_ba08fb9980cc4005a4694935d78500b2",
              "IPY_MODEL_8537b92468524e4cb9f2e9359431b0d2"
            ],
            "layout": "IPY_MODEL_65e6bb001c714b459e98f27df74343a5"
          }
        },
        "3af44a57f6fd4892935bcc076485b52a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e227659d780402db14dca67f1c6895a",
            "placeholder": "",
            "style": "IPY_MODEL_f7cc9c9fbe5c4c649dcc6081ec62d48d",
            "value": "special_tokens_map.json:100%"
          }
        },
        "ba08fb9980cc4005a4694935d78500b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8bb52bda28842fba2e986a70f3ebc02",
            "max": 978,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa7d4bfd6ef44083ac6fa17ba73ae77b",
            "value": 978
          }
        },
        "8537b92468524e4cb9f2e9359431b0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8db1c5d30f8f46d3959fe45526205da4",
            "placeholder": "",
            "style": "IPY_MODEL_9524fe27c45b4a5d8f2eeb169ddfa7fc",
            "value": "978/978[00:00&lt;00:00,71.4kB/s]"
          }
        },
        "65e6bb001c714b459e98f27df74343a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e227659d780402db14dca67f1c6895a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7cc9c9fbe5c4c649dcc6081ec62d48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8bb52bda28842fba2e986a70f3ebc02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7d4bfd6ef44083ac6fa17ba73ae77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8db1c5d30f8f46d3959fe45526205da4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9524fe27c45b4a5d8f2eeb169ddfa7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73a419ddafc44b16a49c4b3724f308e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cb54dd2e3e9427d8dfbbe47de18683a",
              "IPY_MODEL_20bfb01bcdb34885b2d1b45e62569a73",
              "IPY_MODEL_1f716f96d3344ed68ce2fd4c9124ea5d"
            ],
            "layout": "IPY_MODEL_9a6e4e82e2ff43ab929472454c756588"
          }
        },
        "1cb54dd2e3e9427d8dfbbe47de18683a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bce2d7fcbad2410abdaffb059083f917",
            "placeholder": "",
            "style": "IPY_MODEL_f4bbffa9dd2243f5a187b7b67370520b",
            "value": "tokenizer.json:"
          }
        },
        "20bfb01bcdb34885b2d1b45e62569a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_898bcd8e609f4e9bb712d44293154b4e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_417c1ecd2b7745c287c15be9da7f5238",
            "value": 1
          }
        },
        "1f716f96d3344ed68ce2fd4c9124ea5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d26abe36907645c3b2ecc4905f6e39d8",
            "placeholder": "",
            "style": "IPY_MODEL_adbb05ce8c4d4dd8b356ced8ddb1dd7e",
            "value": "2.10M/?[00:00&lt;00:00,155kB/s]"
          }
        },
        "9a6e4e82e2ff43ab929472454c756588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bce2d7fcbad2410abdaffb059083f917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4bbffa9dd2243f5a187b7b67370520b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "898bcd8e609f4e9bb712d44293154b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "417c1ecd2b7745c287c15be9da7f5238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d26abe36907645c3b2ecc4905f6e39d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adbb05ce8c4d4dd8b356ced8ddb1dd7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da6e378d5fe64ec3bb0758f6595c6ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ad03fdc3cec471e8fb581435cefab82",
              "IPY_MODEL_5d8285a7a5b943d8900aaca4233d1b3c",
              "IPY_MODEL_e2276b1005164826ad5a65b5ed7eefaf"
            ],
            "layout": "IPY_MODEL_cd3769f470da42898b8ab168ab56e60c"
          }
        },
        "1ad03fdc3cec471e8fb581435cefab82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34970ac525d0406fb18ee808aabbf873",
            "placeholder": "",
            "style": "IPY_MODEL_d087f1ad10ff48c4b7219fa53d7dd4e0",
            "value": "Unsloth:Tokenizing[&quot;text&quot;](num_proc=6):100%"
          }
        },
        "5d8285a7a5b943d8900aaca4233d1b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a8f72f37d324efab1b8476b43eaefae",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93d4bf8a68504367bc5b2aff18b747e2",
            "value": 600
          }
        },
        "e2276b1005164826ad5a65b5ed7eefaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4079f8a64ab408eaaa5d0b4b71bd59c",
            "placeholder": "",
            "style": "IPY_MODEL_e9addf9c2bbd43ba8f9968e403505143",
            "value": "600/600[00:01&lt;00:00,703.90examples/s]"
          }
        },
        "cd3769f470da42898b8ab168ab56e60c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34970ac525d0406fb18ee808aabbf873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d087f1ad10ff48c4b7219fa53d7dd4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a8f72f37d324efab1b8476b43eaefae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93d4bf8a68504367bc5b2aff18b747e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4079f8a64ab408eaaa5d0b4b71bd59c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9addf9c2bbd43ba8f9968e403505143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0122288a05f2498eb51772576989849f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33cc249495bf4400870d6d04df73c80c",
              "IPY_MODEL_10f226fa970a4bed93e04104481efa22",
              "IPY_MODEL_a547b724e2f242839e217d34045d5777"
            ],
            "layout": "IPY_MODEL_fe30f6a92a8545aebaeb2c7e5b52d133"
          }
        },
        "33cc249495bf4400870d6d04df73c80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42fc9e010fa54100927d6b5078536d8f",
            "placeholder": "",
            "style": "IPY_MODEL_6b1ec1ebfc7e4988b57cf4c34b76f9b3",
            "value": "Unsloth:Tokenizing[&quot;text&quot;](num_proc=6):100%"
          }
        },
        "10f226fa970a4bed93e04104481efa22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5573f3b3df6d4b159ec883e9d5730220",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e2c297d14b6461384191e470ad29e7b",
            "value": 600
          }
        },
        "a547b724e2f242839e217d34045d5777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5995beb14e86492098084f63d7bb9689",
            "placeholder": "",
            "style": "IPY_MODEL_aa57fba2bbfa44d19fd578f49e6e7c15",
            "value": "600/600[00:01&lt;00:00,421.06examples/s]"
          }
        },
        "fe30f6a92a8545aebaeb2c7e5b52d133": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42fc9e010fa54100927d6b5078536d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1ec1ebfc7e4988b57cf4c34b76f9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5573f3b3df6d4b159ec883e9d5730220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2c297d14b6461384191e470ad29e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5995beb14e86492098084f63d7bb9689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa57fba2bbfa44d19fd578f49e6e7c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}